# Nexus Configuration Example
# Copy this file to nexus.yaml and fill in your credentials

version: 1

server:
  host: 0.0.0.0
  grpc_port: 50051
  http_port: 8080
  metrics_port: 9090

cluster:
  enabled: false
  node_id: ""
  allow_multiple_gateways: false
  session_locks:
    enabled: false
    ttl: 2m
    refresh_interval: 30s
    acquire_timeout: 10s
    poll_interval: 200ms

gateway:
  broadcast:
    # Strategy for broadcast group processing: parallel or sequential.
    strategy: parallel
    # Peer ID -> agent IDs for broadcast routing.
    groups: {}
  webhook_hooks:
    enabled: false
    base_path: /hooks
    token: ${WEBHOOK_HOOKS_TOKEN}
    max_body_bytes: 262144
    mappings:
      - path: agent
        name: agent-webhook
        handler: agent
        agent_id: main
      - path: wake
        name: wake-webhook
        handler: wake
        agent_id: main

canvas_host:
  # Dedicated canvas host for local HTML/JS canvas files.
  # When enabled, the canvas is available at http://<host>:<port>/__nexus__/canvas/
  enabled: false
  # host: 0.0.0.0
  # port: 18793
  # root: ./canvas
  # namespace: /__nexus__
  # live_reload: true
  # inject_client: true
  # auto_index: true
  # a2ui_root: ./a2ui

canvas:
  retention:
    # How long to keep the latest canvas snapshot.
    state_max_age: 720h
    # How long to retain canvas events for replay.
    event_max_age: 168h
    # Maximum size of stored canvas snapshot in bytes.
    state_max_bytes: 1048576
    # Maximum size of stored canvas event payload in bytes.
    event_max_bytes: 262144
  tokens:
    # Secret used to sign canvas access tokens.
    # Generate with: openssl rand -base64 32
    secret: ${CANVAS_TOKEN_SECRET}
    ttl: 30m
  actions:
    # Role applied to authenticated requests without a signed canvas token.
    default_role: viewer
    rate_limit:
      enabled: true
      requests_per_second: 10
      burst_size: 20
  audit:
    enabled: false
    level: info
    format: json
    output: stdout

commands:
  # Enable text command handling
  enabled: true
  # Optional allowlist for command-only messages (empty = allow all)
  allow_from:
    # telegram:
    #   - "12345678"
    #   - "*"
  # Inline command shortcuts require an explicit allowlist
  inline_allow_from:
    # telegram:
    #   - "12345678"
  # Inline commands allowed to run inside normal messages
  inline_commands:
    - help
    - commands
    - status
    - whoami
    - id

database:
  # CockroachDB connection string
  # For local dev: postgres://root@localhost:26257/nexus?sslmode=disable
  # For production: postgres://user:password@host:26257/nexus?sslmode=verify-full
  url: ${DATABASE_URL:-postgres://root@localhost:26257/nexus?sslmode=disable}
  max_connections: 25
  conn_max_lifetime: 5m

auth:
  # Generate with: openssl rand -base64 32
  jwt_secret: ${JWT_SECRET}
  token_expiry: 24h
  # Optional API keys for gRPC access (used when no JWT is provided)
  api_keys:
    - key: ${NEXUS_API_KEY}
      user_id: operator
      name: "Operator key"

  oauth:
    google:
      client_id: ${GOOGLE_CLIENT_ID}
      client_secret: ${GOOGLE_CLIENT_SECRET}
      redirect_url: http://localhost:8080/auth/google/callback
    github:
      client_id: ${GITHUB_CLIENT_ID}
      client_secret: ${GITHUB_CLIENT_SECRET}
      redirect_url: http://localhost:8080/auth/github/callback

session:
  # Default agent ID used for session routing
  default_agent_id: main
  # Session scope for threaded channels: "thread" or "channel"
  slack_scope: thread
  discord_scope: thread
  # Optional local memory log
  memory:
    enabled: false
    directory: memory
    max_lines: 20
    days: 2
    scope: session # session, channel, or global
  # Optional heartbeat checklist file for heartbeat-style runs
  heartbeat:
    enabled: false
    file: HEARTBEAT.md
    mode: always # always or on_demand
  # Optional memory flush reminder when sessions grow large
  memory_flush:
    enabled: false
    threshold: 80
    prompt: "Session nearing compaction. If there are durable facts, store them in memory/YYYY-MM-DD.md or MEMORY.md. Reply NO_REPLY if nothing needs attention."

workspace:
  # Optional workspace bootstrap files (Clawdbot/Clawd-style)
  enabled: false
  path: .
  max_chars: 20000
  agents_file: AGENTS.md
  soul_file: SOUL.md
  user_file: USER.md
  identity_file: IDENTITY.md
  tools_file: TOOLS.md
  memory_file: MEMORY.md

skills:
  sources: []
  load:
    watch: false
    watchDebounceMs: 500
  entries: {}

templates:
  sources: []
  load:
    watch: false
    watchDebounceMs: 500
  entries: {}

marketplace:
  enabled: false
  registries: []
  trusted_keys: {}
  auto_update: false
  check_interval: 24h
  skip_verify: false

identity:
  # Optional agent identity
  name: ""
  creature: ""
  vibe: ""
  emoji: ""

user:
  # Optional user profile
  name: ""
  preferred_address: ""
  pronouns: ""
  timezone: ""
  notes: ""

vector_memory:
  enabled: false
  backend: sqlite-vec # sqlite-vec | lancedb | pgvector
  dimension: 1536
  sqlite_vec:
    path: ~/.nexus/vector-memory.sqlite
  lancedb:
    path: ~/.nexus/lancedb
    index_type: flat
    metric_type: cosine
    n_probes: 10
    ef: 64
    refine_factor: 2
  pgvector:
    dsn: ${VECTOR_MEMORY_DSN:-}
    use_cockroachdb: false
    run_migrations: true
  embeddings:
    provider: openai
    api_key: ${OPENAI_API_KEY}
    base_url: https://api.openai.com/v1
    model: text-embedding-3-small
    ollama_url: http://localhost:11434
    project_id: ""
    location: ""
  indexing:
    auto_index_messages: false
    min_content_length: 10
    batch_size: 100
  search:
    default_limit: 10
    default_threshold: 0.7
    default_scope: session
    hierarchy:
      enabled: false
      scopes: ["session", "agent", "channel", "global"]
      max_results: 10
      weights:
        session: 1.0
        agent: 0.8
        channel: 0.7
        global: 0.5
  consolidation:
    enabled: false
    interval: 6h
    min_messages: 20
    max_messages: 120
    max_sessions: 50
    summary_max_chars: 2000
    summary_max_tokens: 512
    model: ""

rag:
  enabled: false
  store:
    backend: pgvector
    dsn: ${RAG_DSN:-}
    use_database_url: true
    dimension: 1536
    run_migrations: true
  chunking:
    chunk_size: 1000
    chunk_overlap: 200
    min_chunk_size: 100
  embeddings:
    provider: openai
    api_key: ${OPENAI_API_KEY}
    base_url: https://api.openai.com/v1
    model: text-embedding-3-small
    batch_size: 100
  search:
    default_limit: 5
    default_threshold: 0.7
    max_results: 20
  context_injection:
    enabled: false
    max_chunks: 5
    max_tokens: 2000
    min_score: 0.7
    scope: global

attention:
  enabled: false
  inject_in_prompt: true
  max_items: 5

steering:
  enabled: false
  rules:
    - id: "priority-accounts"
      name: "Priority account tone"
      prompt: "When responding to priority accounts, be extra concise and confirm next steps."
      priority: 10
      tags: ["priority"]
      channels: ["slack", "discord"]

channels:
  telegram:
    enabled: true
    # Get from @BotFather on Telegram
    bot_token: ${TELEGRAM_BOT_TOKEN}
    # Optional: webhook URL for production
    # webhook: https://your-domain.com/webhook/telegram
    dm:
      policy: open # open | allowlist | pairing | disabled
      # allow_from: ["123456789"]
    group:
      policy: allowlist
      # allow_from: ["-123456789"]

  discord:
    enabled: true
    # Get from Discord Developer Portal
    bot_token: ${DISCORD_BOT_TOKEN}
    app_id: ${DISCORD_APP_ID}
    dm:
      policy: pairing
      # allow_from: ["1234567890"]
    group:
      policy: allowlist
      # allow_from: ["123456789012345678"]

  slack:
    enabled: true
    # Get from Slack App settings
    bot_token: ${SLACK_BOT_TOKEN}      # xoxb-...
    app_token: ${SLACK_APP_TOKEN}      # xapp-... (for Socket Mode)
    signing_secret: ${SLACK_SIGNING_SECRET}
    dm:
      policy: pairing
      # allow_from: ["U1234567890"]
    group:
      policy: allowlist
      # allow_from: ["C1234567890"]
    canvas:
      enabled: false
      command: /canvas
      shortcut_callback: open_canvas
      # allowed_workspaces: ["T1234567890"]
      # default_role: editor
      # workspace_roles:
      #   T1234567890: admin
      # user_roles:
      #   T1234567890:
      #     U1234567890: viewer

  whatsapp:
    enabled: false
    session_path: ~/.nexus/whatsapp/session.db
    media_path: ~/.nexus/whatsapp/media
    sync_contacts: true
    dm:
      policy: pairing
    group:
      policy: allowlist
    presence:
      send_read_receipts: true
      send_typing: true
      broadcast_online: false

  signal:
    enabled: false
    account: ${SIGNAL_ACCOUNT} # +15555550123
    signal_cli_path: signal-cli
    config_dir: ~/.config/signal-cli
    dm:
      policy: pairing
    group:
      policy: allowlist
    presence:
      send_read_receipts: true
      send_typing: true

  imessage:
    enabled: false
    # macOS only (requires local Messages DB access)
    database_path: ~/Library/Messages/chat.db
    poll_interval: 3s
    dm:
      policy: pairing
    group:
      policy: allowlist

  matrix:
    enabled: false
    homeserver: ${MATRIX_HOMESERVER:-https://matrix.org}
    user_id: ${MATRIX_USER_ID:-}
    access_token: ${MATRIX_ACCESS_TOKEN:-}
    device_id: ${MATRIX_DEVICE_ID:-}
    allowed_rooms: []
    allowed_users: []
    join_on_invite: false
    dm:
      policy: pairing
    group:
      policy: allowlist

  # Optional: Home Assistant conversation + tools integration
  homeassistant:
    enabled: false
    base_url: http://homeassistant:8123
    token: ${HOME_ASSISTANT_TOKEN}
    # timeout: 10s

  teams:
    enabled: false
    tenant_id: ${TEAMS_TENANT_ID}
    client_id: ${TEAMS_CLIENT_ID}
    client_secret: ${TEAMS_CLIENT_SECRET}
    # Optional: webhook URL for Graph subscriptions (recommended)
    # webhook_url: https://your-domain.com/webhook/teams
    # Optional: polling fallback interval (default: 5s)
    # poll_interval: 5s
    dm:
      policy: pairing
    group:
      policy: allowlist

  email:
    enabled: false
    tenant_id: ${EMAIL_TENANT_ID}
    client_id: ${EMAIL_CLIENT_ID}
    client_secret: ${EMAIL_CLIENT_SECRET}
    user_email: ${EMAIL_USER_EMAIL}
    folder_id: "" # default: inbox
    include_read: false
    auto_mark_read: true
    poll_interval: 30s

  mattermost:
    enabled: false
    server_url: ${MATTERMOST_SERVER_URL} # https://mattermost.example.com
    # Provide either a bot token OR username/password
    token: ${MATTERMOST_TOKEN:-}
    # username: ${MATTERMOST_USERNAME:-}
    # password: ${MATTERMOST_PASSWORD:-}
    # team_name: ${MATTERMOST_TEAM:-}
    # rate_limit: 10
    # rate_burst: 5
    dm:
      policy: pairing
    group:
      policy: allowlist

  nextcloud_talk:
    enabled: false
    base_url: ${NEXTCLOUD_BASE_URL} # https://nextcloud.example.com
    bot_secret: ${NEXTCLOUD_TALK_BOT_SECRET}
    # webhook_port: 8788
    # webhook_host: 0.0.0.0
    # webhook_path: /nextcloud-talk-webhook
    # rate_limit: 10
    # rate_burst: 5
    dm:
      policy: pairing
    group:
      policy: allowlist

  zalo:
    enabled: false
    token: ${ZALO_BOT_TOKEN}
    # Optional: webhook URL for production
    # webhook_url: https://your-domain.com/webhook/zalo
    # webhook_secret: ${ZALO_WEBHOOK_SECRET:-}
    # webhook_path: /webhook/zalo
    # poll_timeout: 30
    dm:
      policy: pairing
    group:
      policy: allowlist

  bluebubbles:
    enabled: false
    server_url: ${BLUEBUBBLES_SERVER_URL} # http://mac-host:1234
    password: ${BLUEBUBBLES_PASSWORD}
    # webhook_path: /webhook/bluebubbles
    # timeout: 10s
    dm:
      policy: pairing
    group:
      policy: allowlist

llm:
  # default_provider can reference a profile (e.g., "openai:fast")
  default_provider: anthropic

  providers:
    anthropic:
      api_key: ${ANTHROPIC_API_KEY}
      default_model: claude-sonnet-4-20250514
      # Optional: for high-volume usage
      # base_url: https://api.anthropic.com

    openai:
      api_key: ${OPENAI_API_KEY}
      default_model: gpt-4o
      # Optional: for Azure OpenAI
      # base_url: https://your-resource.openai.azure.com
      profiles:
        fast:
          api_key: ${OPENAI_FAST_API_KEY}
          default_model: gpt-4o-mini

    google:
      api_key: ${GOOGLE_AI_API_KEY}
      default_model: gemini-pro

    openrouter:
      api_key: ${OPENROUTER_API_KEY}
      default_model: anthropic/claude-3.5-sonnet

    azure:
      api_key: ${AZURE_OPENAI_API_KEY}
      base_url: https://your-resource.openai.azure.com
      api_version: ${AZURE_OPENAI_API_VERSION}
      default_model: gpt-4o-deployment

    bedrock:
      default_model: anthropic.claude-3-sonnet-20240229-v1:0

    copilot-proxy:
      base_url: http://localhost:3000/v1
      default_model: gpt-5.2

    # Optional local Ollama provider
    # ollama:
    #   base_url: http://localhost:11434
    #   default_model: llama3

  routing:
    enabled: false
    classifier: heuristic
    prefer_local: false
    # unhealthy_cooldown: 30s
    # rules:
    #   - name: quick
    #     match:
    #       patterns: ["what is", "define", "quick"]
    #     target:
    #       provider: ollama
    #       model: qwen2.5:1.5b
    #   - name: reasoning
    #     match:
    #       tags: ["reasoning"]
    #     target:
    #       provider: anthropic
    #       model: claude-sonnet-4-20250514
    # fallback:
    #   provider: anthropic

  auto_discover:
    ollama:
      enabled: false
      prefer_local: true
      probe_locations:
        - http://localhost:11434
        - http://ollama:11434
        - http://ollama.ollama.svc.cluster.local:11434

  bedrock:
    enabled: false
    region: us-east-1
    refresh_interval: 1h
    provider_filter: []

experiments:
  # Optional prompt/model experiments (A/B testing)
  experiments: []
  # Example:
  # experiments:
  #   - id: system-prompt-v2
  #     description: "Test new system prompt"
  #     status: active
  #     allocation: 20
  #     variants:
  #       - id: control
  #         weight: 50
  #         config:
  #           system_prompt: "You are a helpful assistant."
  #       - id: treatment
  #         weight: 50
  #         config:
  #           system_prompt: "You are concise and direct."

mcp:
  enabled: false
  servers: []
  # Example:
  # servers:
  #   - id: filesystem
  #     name: Local filesystem
  #     transport: stdio
  #     command: mcp-server-filesystem
  #     args: ["--root", "."]
  #     auto_start: false

tools:
  # Optional notes to include in the system prompt for tool conventions
  notes: ""
  # Optional file path with tool notes (merged with inline notes)
  notes_file: ""
  # Optional memory search tool
  memory_search:
    enabled: false
    directory: memory
    memory_file: MEMORY.md
    max_results: 5
    max_snippet_len: 200
    mode: hybrid # lexical, vector, or hybrid
    embeddings:
      provider: openai
      api_key: ${OPENAI_API_KEY}
      base_url: https://api.openai.com/v1
      model: text-embedding-3-small
      cache_dir: ~/.nexus/cache/embeddings
      cache_ttl: 24h
      timeout: 15s
  fact_extraction:
    enabled: false
    max_facts: 10
  links:
    enabled: false
    max_links: 5
    timeout_seconds: 30
    models: []
    scope:
      mode: all # all | allowlist | denylist
      allowlist: []
      denylist: []
  sandbox:
    enabled: true
    backend: docker # docker | firecracker
    # Number of pre-warmed sandboxes
    pool_size: 5
    max_pool_size: 10
    min_idle: 5
    max_idle_time: 5m
    # Maximum execution time per request
    timeout: 30s
    network_enabled: false
    limits:
      max_cpu: 1000 # millicores
      max_memory: 512MB
    snapshots:
      enabled: false
      refresh_interval: 30m
      max_age: 6h
  browser:
    enabled: true
    headless: true
    # Optional: connect to remote Playwright server
    # url: http://playwright:3000
  computer_use:
    enabled: false
    # Default edge to target for computer use (optional)
    edge_id: ""
    # Optional display overrides when node metadata is unavailable
    display_width_px: 0
    display_height_px: 0
    display_number: 0
  websearch:
    enabled: true
    provider: searxng
    # SearXNG instance URL
    url: ${SEARXNG_URL:-http://localhost:8888}
  web_fetch:
    enabled: true
    max_chars: 10000
  execution:
    max_iterations: 4
    parallelism: 2
    timeout: 0s
    max_attempts: 1
    retry_backoff: 0s
    disable_events: false
    max_tool_calls: 0
    require_approval: []
    approval:
      profile: coding
      allowlist: []
      denylist: []
      safe_bins: []
      skill_allowlist: true
      ask_fallback: true
      default_decision: pending
      request_ttl: 5m
    result_guard:
      enabled: false
      max_chars: 0
      denylist: []
      redact_patterns: []
      redaction_text: "[redacted]"
      truncate_suffix: "...[truncated]"
      sanitize_secrets: true
    async: []
  jobs:
    retention: 24h
    prune_interval: 1h
  elevated:
    enabled: false
    allow_from:
      telegram: ["123456789"]
    tools: ["execute_code"]

  servicenow:
    enabled: false
    instance_url: ${SERVICENOW_INSTANCE_URL}
    username: ${SERVICENOW_USERNAME}
    password: ${SERVICENOW_PASSWORD}

edge:
  enabled: false
  auth_mode: token # token | tofu | dev
  tokens: {}
  heartbeat_interval: 30s
  heartbeat_timeout: 2m
  default_tool_timeout: 60s
  max_concurrent_tools: 10
  event_buffer_size: 1000

artifacts:
  # Storage backend: local | s3 | minio | none
  backend: local
  # Directory for local storage and metadata
  local_path: /tmp/nexus-artifacts
  # Optional metadata file path (defaults to <local_path>/metadata.json)
  metadata_path: /tmp/nexus-artifacts/metadata.json
  # Metadata backend: file | database
  metadata_backend: file
  # S3/MinIO settings (used when backend is s3/minio)
  s3_bucket: ${NEXUS_ARTIFACTS_BUCKET}
  s3_endpoint: ${NEXUS_ARTIFACTS_ENDPOINT}
  s3_region: ${AWS_REGION:-us-east-1}
  # Cleanup interval for expired artifacts
  prune_interval: 1h
  # TTLs by artifact type
  ttls:
    screenshot: 168h
    recording: 720h
    file: 336h
    default: 24h
  redaction:
    enabled: false
    types: []
    mime_types: []
    filename_patterns: []

transcription:
  enabled: false
  provider: openai
  api_key: ${OPENAI_API_KEY}
  base_url: https://api.openai.com/v1
  model: whisper-1
  language: ""

tts:
  # When enabled, Nexus can generate audio responses (e.g., for voice messages).
  enabled: false
  # provider: macos | edge | openai | elevenlabs
  provider: macos
  # fallback_chain: [edge, openai]
  fallback_chain: [edge]
  max_text_length: 4096
  timeout_seconds: 30
  # output_dir: /tmp/nexus-tts
  macos:
    voice: ""
    rate: 0
  edge:
    voice: en-US-AriaNeural
    output_format: audio-24khz-48kbitrate-mono-mp3
  openai:
    api_key: ${OPENAI_API_KEY}
    model: tts-1
    voice: alloy
    response_format: mp3
    speed: 1.0
    base_url: https://api.openai.com/v1
  elevenlabs:
    api_key: ${ELEVENLABS_API_KEY}
    voice_id: 21m00Tcm4TlvDq8ikWAM
    model_id: eleven_monolingual_v1
    output_format: mp3_44100_128
    stability: 0.5
    similarity_boost: 0.75

cron:
  # Config-driven cron scheduler (job types: message, webhook, agent, custom).
  enabled: false
  jobs: []
  # Example jobs:
  # - id: daily-reminder
  #   name: Daily reminder
  #   type: message
  #   enabled: true
  #   schedule:
  #     cron: "0 9 * * 1-5"
  #     timezone: "America/New_York"
  #   message:
  #     channel: slack
  #     channel_id: U123456
  #     content: "Standup in 10 minutes."
  #     # template: "Standup in 10 minutes on {{.date}} for {{.team}}."
  #     # data:
  #     #   team: Eng
  #
  # - id: weekly-digest
  #   name: Weekly digest
  #   type: agent
  #   enabled: true
  #   schedule:
  #     cron: "0 18 * * 5"
  #     timezone: "America/New_York"
  #   message:
  #     # channel/channel_id are optional for agent jobs. When omitted, the gateway
  #     # defaults to `channel=api` and `channel_id=cron:<job-id>`.
  #     channel: slack
  #     channel_id: C123456
  #     content: "Generate a weekly digest of key discussions and action items."
  #     # tools:
  #     #   - web_search
  #     #   - read
  #   # retry:
  #   #   max_retries: 2
  #   #   backoff: 30s
  #   #   max_backoff: 5m
  #
  # - id: ping-webhook
  #   name: Ping webhook
  #   type: webhook
  #   enabled: true
  #   schedule:
  #     every: 1h
  #   webhook:
  #     url: https://example.com/ping
  #     method: POST
  #     headers:
  #       Content-Type: application/json
  #     body: '{"source":"nexus"}'
  #     timeout: 30s
  #     # auth:
  #     #   type: bearer
  #     #   token: "${WEBHOOK_TOKEN}"
  #
  # - id: nightly-maintenance
  #   name: Custom maintenance job
  #   type: custom
  #   enabled: true
  #   schedule:
  #     at: "02:00"
  #   custom:
  #     handler: maintenance
  #     args:
  #       scope: sessions

tasks:
  # Scheduled task system (DB-backed task definitions + executions).
  enabled: false
  worker_id: ""
  poll_interval: 10s
  acquire_interval: 1s
  lock_duration: 10m
  max_concurrency: 5
  cleanup_interval: 1m
  stale_timeout: 30m
  default_timeout: 5m

plugins:
  # Optional plugin manifest search paths
  load:
    paths: []
  # Optional (future): run third-party plugins out-of-process. Today this only
  # logs a warning and plugins still run in-process.
  isolation:
    enabled: false
    backend: docker # docker | firecracker
    network_enabled: false
    timeout: 30s
    limits:
      max_cpu: 1000 # millicores
      max_memory: 256MB
  # Example plugin entry
  # entries:
  #   voice-call:
  #     enabled: true
  #     path: ./plugins/voice-call # directory containing nexus.plugin.json + plugin.so (or <id>.so)
  #     config:
  #       token: ${VOICE_CALL_TOKEN}
  #   sample-echo:
  #     enabled: true
  #     path: ./examples/plugins/echo
  #     config:
  #       prefix: "[echo] "

logging:
  level: info  # debug, info, warn, error
  format: json # json, text
  # Optional: output to file
  # file: /var/log/nexus/nexus.log

observability:
  tracing:
    enabled: false
    endpoint: ""
    service_name: nexus
    service_version: "dev"
    environment: "local"
    sampling_rate: 1.0
    insecure: true
    attributes: {}

security:
  posture:
    enabled: false
    interval: 10m
    include_filesystem: true
    include_gateway: true
    include_config: true
    check_symlinks: true
    allow_group_readable: false
    emit_events: true
    auto_remediation:
      enabled: false
      mode: warn_only
